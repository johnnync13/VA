{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------\n",
    "----------------------------------------------------------------------------------------------------\n",
    "<h1><center>\n",
    "    \n",
    "Final exam version B (Evaluación Unica)\n",
    " \n",
    "( 9th of January, 2020 )\n",
    "</center></h1>    \n",
    "\n",
    "----------------------------------------------------------------------------------------------------\n",
    "----------------------------------------------------------------------------------------------------\n",
    "\n",
    "Please read the exercices carefully, write the necesary code and respond to all the questions. The code needs to be properly commented. Figures must include labels/titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "**(1.5 points)** - Create an image (500 x 600 pixels) with three overlapped circles as shown below. The centre of the circles are located at (200,200), (300,300) and (400,200), and the radius is 150 pixels for all of them. You might want to use the library `numpy.ogrid` to create the circles. Plot the resulted image. \n",
    "\n",
    "<img src=\"images/model_RGB.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 (2 points)\n",
    "\n",
    "**(a) (0.75 points)** Load the image stored in the `images/circles.png` file. Apply thresholding over the blue-channel image to detect the blue circles as observed in the bottom image.\n",
    "\n",
    "<img src=\"images/Ex2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) (0.75 points)** Try the `felzenszwalb` and `SLIC` segmentation methods over the original image. Play with the parameters to optimise the segmentation. Display in a 1x2 plot the segmented images. Add titles to the figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) (0.5 points)** Provide the number of circles segmented for each method. Which one perfom better results? Discuss the performance of each of the methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (1.5 points)\n",
    "**(a) (0.75 points)** - Load the image stored in the `images/lena.png` file. Add `salt and peper` and `gaussian` noise to the image uing the library `skimage.util.random_noise`. Display the original and the noisy images in a 1x3 plot. You can wok in the gray scale of the image.\n",
    "\n",
    "<img src=\"images/lena.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "**(b) (0.75 points)** Apply `Gaussian` and `median` filters to each of the noisy images (`Gaussian` and `Salt and Pepper`). Plot the results in a 2x3 plot showing the Original, Gaussian smooth and median smooth images. Explain the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4  (1.75 points)\n",
    "\n",
    "Given a query image `images/template.png` and a target image `images/target_image.png`, find the location of the query within the target image. \n",
    "\n",
    "<img src=\"images/best_match.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) (0.75 Points)** Perform a sliding windows of a query image `images/template.png` over the target image `images/target_image.png` to detect the location of the query in the image using the normalized cross-correlation as similarity metric (hint: `match_template() of skimage.feature')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) (1.0 Point)** Repeat the above detection using HOG. Perform a sliding windows over the target image and obtain the HOG descriptor for each region. Then compare with the HOG descriptor of the query image using an appropriate similarity measure, and show the location with highest similarity.\n",
    "\n",
    "In order to accelerate algorithm execution, you can apply a sliding window with a step of 10 pixels both vertically and horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5  \n",
    "\n",
    "**(1.5 Points)** Use the ORB feature descriptor to compare the given query image (`/images/amazon_template.jpg`) to the given logo collection in the `log_collection folder`. Then, sort them out based on their similarity (i.e number of match points) to the query image. \n",
    "\n",
    "**Hint:** `ORB` is a function within the module `skimage.feature`. You can make use of the following given function to visualize the matches among images.\n",
    "\n",
    "<img src=\"images/amazon_template.jpg\" width=\"200\" height=\"20\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case the plot_matches() function gives you some problems, you can use the following one:\n",
    "\n",
    "from skimage.util import img_as_float\n",
    "import numpy as np\n",
    "\n",
    "def plot_matches_aux(ax, image1, image2, keypoints1, keypoints2, matches,\n",
    "                 keypoints_color='k', matches_color=None, only_matches=False):\n",
    "    \"\"\"Plot matched features.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        Matches and image are drawn in this ax.\n",
    "    image1 : (N, M [, 3]) array\n",
    "        First grayscale or color image.\n",
    "    image2 : (N, M [, 3]) array\n",
    "        Second grayscale or color image.\n",
    "    keypoints1 : (K1, 2) array\n",
    "        First keypoint coordinates as ``(row, col)``.\n",
    "    keypoints2 : (K2, 2) array\n",
    "        Second keypoint coordinates as ``(row, col)``.\n",
    "    matches : (Q, 2) array\n",
    "        Indices of corresponding matches in first and second set of\n",
    "        descriptors, where ``matches[:, 0]`` denote the indices in the first\n",
    "        and ``matches[:, 1]`` the indices in the second set of descriptors.\n",
    "    keypoints_color : matplotlib color, optional\n",
    "        Color for keypoint locations.\n",
    "    matches_color : matplotlib color, optional\n",
    "        Color for lines which connect keypoint matches. By default the\n",
    "        color is chosen ra£ndomly.\n",
    "    only_matches : bool, optional\n",
    "        Whether to only plot matches and not plot the keypoint locations.\n",
    "    \"\"\"\n",
    "\n",
    "    image1 = img_as_float(image1)\n",
    "    image2 = img_as_float(image2)\n",
    "\n",
    "    new_shape1 = list(image1.shape)\n",
    "    new_shape2 = list(image2.shape)\n",
    "\n",
    "    if image1.shape[0] < image2.shape[0]:\n",
    "        new_shape1[0] = image2.shape[0]\n",
    "    elif image1.shape[0] > image2.shape[0]:\n",
    "        new_shape2[0] = image1.shape[0]\n",
    "\n",
    "    if image1.shape[1] < image2.shape[1]:\n",
    "        new_shape1[1] = image2.shape[1]\n",
    "    elif image1.shape[1] > image2.shape[1]:\n",
    "        new_shape2[1] = image1.shape[1]\n",
    "\n",
    "    if new_shape1 != image1.shape:\n",
    "        new_image1 = np.zeros(new_shape1, dtype=image1.dtype)\n",
    "        new_image1[:image1.shape[0], :image1.shape[1]] = image1\n",
    "        image1 = new_image1\n",
    "\n",
    "    if new_shape2 != image2.shape:\n",
    "        new_image2 = np.zeros(new_shape2, dtype=image2.dtype)\n",
    "        new_image2[:image2.shape[0], :image2.shape[1]] = image2\n",
    "        image2 = new_image2\n",
    "\n",
    "    image = np.concatenate([image1, image2], axis=1)\n",
    "\n",
    "    offset = image1.shape\n",
    "\n",
    "    if not only_matches:\n",
    "        ax.scatter(keypoints1[:, 1], keypoints1[:, 0],\n",
    "                   facecolors='none', edgecolors=keypoints_color)\n",
    "        ax.scatter(keypoints2[:, 1] + offset[1], keypoints2[:, 0],\n",
    "                   facecolors='none', edgecolors=keypoints_color)\n",
    "\n",
    "    ax.imshow(image, interpolation='nearest', cmap='gray')\n",
    "    ax.axis((0, 2 * offset[1], offset[0], 0))\n",
    "\n",
    "    for i in range(matches.shape[0]):\n",
    "        idx1 = matches[i, 0]\n",
    "        idx2 = matches[i, 1]\n",
    "\n",
    "        if matches_color is None:\n",
    "            color = np.random.rand(3)\n",
    "        else:\n",
    "            color = matches_color\n",
    "\n",
    "        ax.plot((keypoints1[idx1, 1], keypoints2[idx2, 1] + offset[1]),\n",
    "                (keypoints1[idx1, 0], keypoints2[idx2, 0]),\n",
    "                '-', color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6  (1.75 points)\n",
    "\n",
    "Extract features from a set of images, train an AdaBoost classifier and evaluate its performance. The set of images consists of 200 training samples and 100 testing samples representing faces (positive samples) and non-faces (negative samples). The code to load the training and testing images is provided below. Complete all sections from `a)` to `d)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy\n",
    "import skimage\n",
    "from skimage import io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "\n",
    "pos_path_train = \"./images/trainingdata/faces/\"\n",
    "neg_path_train = \"./images/trainingdata/nonfaces/\"\n",
    "\n",
    "pos_path_test = \"./images/trainingdata/faces/test/\"\n",
    "neg_path_test = \"./images/trainingdata/nonfaces/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImages(path):\n",
    "    imgs_list = [io.imread(path+f) for f in os.listdir(path) if \".png\" in f]\n",
    "    \n",
    "    # Do not change this line! Images must be resized for the features calculation to taking less time.\n",
    "    #imgs_list = [resize(im, (50,50)) for im in imgs_list]\n",
    "    return imgs_list\n",
    "\n",
    "def plotImages(img1, img2):\n",
    "    fig, ax= plt.subplots(ncols=2, nrows=1, figsize=(20,30))\n",
    "\n",
    "    ax[0].imshow(img1)\n",
    "    ax[1].imshow(img2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training images\n",
    "pos_images_train = loadImages(pos_path_train)\n",
    "neg_images_train = loadImages(neg_path_train)\n",
    "# Load testing images\n",
    "pos_images_test = loadImages(pos_path_test)\n",
    "neg_images_test = loadImages(neg_path_test)\n",
    "\n",
    "plotImages(pos_images_train[0], neg_images_train[0])\n",
    "\n",
    "# Training set\n",
    "print(len(pos_images_train), len(neg_images_train))\n",
    "# Testing set\n",
    "print(len(pos_images_test), len(neg_images_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) (0.5 points)** Load the filter banks provided and extract features on the grayscale version of the `train` and `test` images.\n",
    "\n",
    "You should have a resulting matrix for the training set with a size of (200, 49), 200 images and 49 features per image. And a resulting matrix for the test set with a size of (100, 49), 100 images and 49 features per images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import filters\n",
    "import LM_filters\n",
    "filter_bank = LM_filters.makeLMfilters()\n",
    "\n",
    "# TRAINING set\n",
    "# Join all images in a list\n",
    "train_images = neg_images_train + pos_images_train\n",
    "# Obtain class labels\n",
    "train_labels = list(np.zeros(len(neg_images_train))) + list(np.ones(len(pos_images_train)))\n",
    "train_labels = [int(l) for l in train_labels]\n",
    "\n",
    "# TESTING set\n",
    "# Join all images in a list\n",
    "test_images = neg_images_test + pos_images_test\n",
    "# Obtain class labels\n",
    "test_labels = list(np.zeros(len(neg_images_test))) + list(np.ones(len(pos_images_test)))\n",
    "test_labels = [int(l) for l in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_from_filter_bank(image, filter_bank, n_filters):\n",
    "    # COMPLETE\n",
    "\n",
    "    return features_for_im\n",
    "\n",
    "def getImageFeatures(all_images,  filter_bank):\n",
    "    # COMPLETE\n",
    "        \n",
    "    return all_feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_features = getImageFeatures(train_images, filter_bank)\n",
    "test_features = getImageFeatures(test_images, filter_bank)\n",
    "\n",
    "# It takes around 6 min to extract the features of both train and test samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) (0.5 points)** We will use the class `sklearn.ensemble.AdaBoostClassifier` to implement a classifier for predicting the output of new samples. The class is used to train the classifier, and after that, to generate the probability that a given sample corresponds to a face. If the probability is above a given threshold, then the sample is classified as a face.\n",
    "\n",
    "The implementation is given below, however, you should complete the code of two functions:\n",
    "\n",
    "- `predLabelsFromFaceProbs(isFaceProbs, threshold)`: given a list with the probability of each sample being a face, this function returns a list of labels for the samples. For a given sample, the label will be 1 if the corresponding probability is above the threshold and zero otherwise.\n",
    "\n",
    "- `predictAdaboost(classifier, features, threshold)`: this function obtains the predicted classes for a list of features given a classifier, a list of features for each sample and a threshold. For this purpose, the probability of face for each sample should be obtained. Once we have the probabilities, they are used to predict the labels according to the given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def trainAdaboost(features, true_labels):\n",
    "    classifier = AdaBoostClassifier()\n",
    "    classifier.fit(features, true_labels)\n",
    "    return classifier\n",
    "\n",
    "def getProbsAdaboost(classifier, features):\n",
    "    return classifier.predict_proba(features)\n",
    "\n",
    "def getIsFaceProbs(classifier, features):\n",
    "    probsAdaboost = getProbsAdaboost(classifier, features)\n",
    "    isFaceProbs = []\n",
    "    for p in probsAdaboost:\n",
    "        isFaceProbs.append(p[1])\n",
    "    return isFaceProbs\n",
    "    \n",
    "def predLabelsFromFaceProbs(isFaceProbs, threshold=0.5):\n",
    "    labels = []\n",
    "    # COMPLETE\n",
    "\n",
    "    return labels\n",
    "\n",
    "def predictAdaboost(classifier, features, threshold=0.5):\n",
    "    # COMPLETE\n",
    "\n",
    "adaboost = trainAdaboost(train_features, train_labels)\n",
    "\n",
    "threshold = 0.5\n",
    "predicted_train_labels = predictAdaboost(adaboost, train_features, threshold)\n",
    "predicted_test_labels = predictAdaboost(adaboost, test_features, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) (0.75)** Implement a function for calculating the prediction sensitivity given a set of true labels and a set of predicted labels. Sensitivity is obtained as the number correctly classified faces divided by the number of true faces (remember, faces have 1 as label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity(true_labels, predicted_labels):\n",
    "    # COMPLETE\n",
    "\n",
    "    return sen\n",
    "\n",
    "# Print training sensitivity\n",
    "# COMPLETE\n",
    "# Print testing sensitivity\n",
    "# COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
